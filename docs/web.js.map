{
  "version": 3,
  "sources": ["../src/index.ts", "../src/utils/concat-uint8-array.ts", "../src/utils/concat-bytes.ts", "../src/utils/limit-bytes.ts", "../src/utils/collect-bytes.ts", "../src/utils/collect-file.ts", "../src/utils/iterator-to-stream.ts", "../src/utils/streamed-file.ts", "../src/utils/iterable-stream-polyfill.ts", "../src/utils/filter-stream.ts", "../src/utils/prepend-bytes.ts", "../src/utils/append-bytes.ts", "../src/zip/decode-zip.ts", "../src/zip/semaphore.ts", "../src/zip/decode-remote-zip.ts", "../src/zip/encode-zip.ts", "../src/web.ts"],
  "sourcesContent": ["export { collectBytes } from './utils/collect-bytes'\nexport { collectFile } from './utils/collect-file'\nexport { iteratorToStream } from './utils/iterator-to-stream'\nexport { StreamedFile } from './utils/streamed-file'\nexport { encodeZip, decodeZip, decodeRemoteZip } from './zip'\n", "/**\n * Concatenates multiple Uint8Arrays into a single Uint8Array.\n *\n * @param arrays The arrays to concatenate.\n * @returns A new Uint8Array containing the contents of all the arrays.\n */\nexport function concatUint8Array(...arrays: Uint8Array[]) {\n  const result = new Uint8Array(\n    arrays.reduce((sum, array) => sum + array.length, 0),\n  )\n  let offset = 0\n  for (const array of arrays) {\n    result.set(array, offset)\n    offset += array.length\n  }\n  return result\n}\n", "import { concatUint8Array } from './concat-uint8-array'\n\n/**\n * Concatenates the contents of the stream into a single Uint8Array.\n *\n * @param totalBytes Optional. The number of bytes to concatenate. Used to\n *  \t\t\t\t pre-allocate the buffer. If not provided, the buffer will\n * \t\t\t\t     be dynamically resized as needed.\n * @returns A stream that will emit a single UInt8Array entry before closing.\n */\nexport function concatBytes(totalBytes?: number) {\n  if (totalBytes === undefined) {\n    let acc = new Uint8Array()\n    return new TransformStream<Uint8Array, Uint8Array>({\n      transform(chunk) {\n        acc = concatUint8Array(acc, chunk)\n      },\n\n      flush(controller) {\n        controller.enqueue(acc)\n      },\n    })\n  } else {\n    const buffer = new ArrayBuffer(totalBytes || 0)\n    let offset = 0\n    return new TransformStream<Uint8Array, Uint8Array>({\n      transform(chunk) {\n        const view = new Uint8Array(buffer)\n        view.set(chunk, offset)\n        offset += chunk.byteLength\n      },\n\n      flush(controller) {\n        controller.enqueue(new Uint8Array(buffer))\n      },\n    })\n  }\n}\n", "/**\n * Limit the number of bytes read from a stream.\n *\n * @param stream The stream to limit.\n * @param bytes The number of bytes to read from the stream.\n * @returns A new stream that will read at most `bytes` bytes from `stream`.\n */\nexport function limitBytes(stream: ReadableStream<Uint8Array>, bytes: number) {\n  if (bytes === 0) {\n    return new ReadableStream({\n      start(controller) {\n        controller.close()\n      },\n    })\n  }\n  const reader = stream.getReader({ mode: 'byob' })\n  let offset = 0\n  return new ReadableStream({\n    async pull(controller) {\n      const { value, done } = await reader.read(new Uint8Array(bytes - offset))\n      if (done) {\n        reader.releaseLock()\n        controller.close()\n        return\n      }\n      offset += value.length\n      controller.enqueue(value)\n\n      if (offset >= bytes) {\n        reader.releaseLock()\n        controller.close()\n      }\n    },\n    cancel() {\n      reader.cancel()\n    },\n  })\n}\n", "import { concatBytes } from './concat-bytes'\nimport { limitBytes } from './limit-bytes'\n\n/**\n * Collects the contents of the entire stream into a single Uint8Array.\n *\n * @param stream The stream to collect.\n * @param bytes Optional. The number of bytes to read from the stream.\n * @returns The string contents of the stream.\n */\nexport async function collectBytes(\n  stream: ReadableStream<Uint8Array>,\n  bytes?: number,\n) {\n  if (bytes !== undefined) {\n    stream = limitBytes(stream, bytes)\n  }\n\n  return await stream\n    .pipeThrough(concatBytes(bytes))\n    .getReader()\n    .read()\n    .then(({ value }) => value!)\n}\n", "import { collectBytes } from './collect-bytes'\n\n/**\n * Collects the contents of the entire stream into a single File object.\n *\n * @param stream The stream to collect.\n * @param fileName The name of the file\n * @returns The string contents of the stream.\n */\nexport async function collectFile(\n  fileName: string,\n  stream: ReadableStream<Uint8Array>,\n) {\n  // @TODO: use StreamingFile\n  return new File([await collectBytes(stream)], fileName)\n}\n", "import { IterableReadableStream } from './iterable-stream-polyfill'\n\n/**\n * Converts an iterator or iterable to a stream.\n *\n * @param iteratorOrIterable The iterator or iterable to convert.\n * @returns A stream that will yield the values from the iterator or iterable.\n */\nexport function iteratorToStream<T>(\n  iteratorOrIterable:\n    | AsyncIterator<T>\n    | Iterator<T>\n    | AsyncIterable<T>\n    | Iterable<T>,\n) {\n  if (iteratorOrIterable instanceof ReadableStream) {\n    return iteratorOrIterable as IterableReadableStream<T>\n  }\n\n  let iterator: AsyncIterator<T> | Iterator<T>\n  if (Symbol.asyncIterator in iteratorOrIterable) {\n    iterator = iteratorOrIterable[Symbol.asyncIterator]()\n  } else if (Symbol.iterator in iteratorOrIterable) {\n    iterator = iteratorOrIterable[Symbol.iterator]()\n  } else {\n    iterator = iteratorOrIterable\n  }\n\n  return new ReadableStream<T>({\n    async pull(controller) {\n      const { done, value } = await iterator.next()\n      if (done) {\n        controller.close()\n        return\n      }\n      controller.enqueue(value)\n    },\n  }) as IterableReadableStream<T>\n}\n", "import { collectBytes } from './collect-bytes'\n\n/**\n * Represents a file that is streamed and not fully\n * loaded into memory.\n */\nexport class StreamedFile extends File {\n  /**\n   * Creates a new StreamedFile instance.\n   *\n   * @param readableStream The readable stream containing the file data.\n   * @param name The name of the file.\n   * @param type The MIME type of the file.\n   */\n  constructor(\n    private readableStream: ReadableStream<Uint8Array>,\n    name: string,\n    type?: string,\n  ) {\n    super([], name, { type })\n  }\n\n  /**\n   * Overrides the slice() method of the File class.\n   *\n   * @returns A Blob representing a portion of the file.\n   */\n  override slice(): Blob {\n    throw new Error('slice() is not possible on a StreamedFile')\n  }\n\n  /**\n   * Returns the readable stream associated with the file.\n   *\n   * @returns The readable stream.\n   */\n  override stream() {\n    return this.readableStream\n  }\n\n  /**\n   * Loads the file data into memory and then returns it as a string.\n   *\n   * @returns File data as text.\n   */\n  override async text() {\n    return new TextDecoder().decode(await this.arrayBuffer())\n  }\n\n  /**\n   * Loads the file data into memory and then returns it as an ArrayBuffer.\n   *\n   * @returns File data as an ArrayBuffer.\n   */\n  override async arrayBuffer() {\n    return (await collectBytes(this.stream())).buffer as ArrayBuffer\n  }\n}\n", "/**\n * Polyfill for ReadableStream[Symbol.asyncIterator]\n * This enables the use of for-await-of loops with ReadableStreams\n *\n * @example\n * ```ts\n * for await (const entry of stream) {\n * \t   // ...\n * }\n * ```\n */\n// @ts-ignore\nif (!ReadableStream.prototype[Symbol.asyncIterator]) {\n  // @ts-ignore\n  ReadableStream.prototype[Symbol.asyncIterator] = async function* () {\n    const reader = this.getReader()\n    try {\n      while (true) {\n        const { done, value } = await reader.read()\n        if (done) {\n          return\n        }\n        yield value\n      }\n    } finally {\n      reader.releaseLock()\n    }\n  }\n  // @ts-ignore\n  ReadableStream.prototype.iterate =\n    // @ts-ignore\n    ReadableStream.prototype[Symbol.asyncIterator]\n}\n\nexport type IterableReadableStream<R> = ReadableStream<R> & AsyncIterable<R>\n", "/**\n * Filter the stream based on a predicate.\n *\n * @param predicate The predicate to filter the stream with.\n * @returns A new stream that will only contain chunks that pass the predicate.\n */\nexport function filterStream<T>(predicate: (chunk: T) => boolean) {\n  return new TransformStream<T, T>({\n    transform(chunk, controller) {\n      if (predicate(chunk)) {\n        controller.enqueue(chunk)\n      }\n    },\n  })\n}\n", "/**\n * Prepend bytes to a stream.\n *\n * @param bytes The bytes to prepend.\n * @returns A transform stream that will prepend the specified bytes.\n */\nexport function prependBytes(bytes: Uint8Array) {\n  let isPrepended = false\n  return new TransformStream<Uint8Array, Uint8Array>({\n    async transform(chunk, controller) {\n      if (!isPrepended) {\n        isPrepended = true\n        controller.enqueue(bytes)\n      }\n      controller.enqueue(chunk)\n    },\n  })\n}\n", "/**\n * Appends bytes to a stream.\n *\n * @param bytes The bytes to append.\n * @returns A transform stream that will append the specified bytes.\n */\nexport function appendBytes(bytes: Uint8Array) {\n  return new TransformStream<Uint8Array, Uint8Array>({\n    async transform(chunk, controller) {\n      controller.enqueue(chunk)\n    },\n    async flush(controller) {\n      controller.enqueue(bytes)\n    },\n  })\n}\n", "/**\n * Reads files from a stream of zip file bytes.\n */\nimport { IterableReadableStream } from '../utils/iterable-stream-polyfill'\n\nimport {\n  SIGNATURE_FILE,\n  SIGNATURE_CENTRAL_DIRECTORY,\n  SIGNATURE_CENTRAL_DIRECTORY_END,\n  FILE_HEADER_SIZE,\n  COMPRESSION_DEFLATE,\n  CompressionMethod,\n} from './types'\nimport {\n  CentralDirectoryEntry,\n  FileEntry,\n  ZipEntry,\n  CentralDirectoryEndEntry,\n} from './types'\nimport { filterStream } from '../utils/filter-stream'\nimport { collectBytes } from '../utils/collect-bytes'\nimport { limitBytes } from '../utils/limit-bytes'\nimport { concatBytes } from '../utils/concat-bytes'\nimport { prependBytes } from '../utils/prepend-bytes'\nimport { appendBytes } from '../utils/append-bytes'\n\n/**\n * Unzips a stream of zip file bytes.\n *\n * @param stream A stream of zip file bytes.\n * @param predicate Optional. A function that returns true if the file should be downloaded.\n * @returns An iterable stream of File objects.\n */\nexport function decodeZip(\n  stream: ReadableStream<Uint8Array>,\n  predicate?: () => boolean,\n) {\n  return streamZippedFileEntries(stream, predicate).pipeThrough(\n    new TransformStream<FileEntry, File>({\n      async transform(zipEntry, controller) {\n        const file = new File(\n          [zipEntry.bytes],\n          new TextDecoder().decode(zipEntry.path),\n          {\n            type: zipEntry.isDirectory ? 'directory' : undefined,\n          },\n        )\n        controller.enqueue(file)\n      },\n    }),\n  ) as IterableReadableStream<File>\n}\n\nconst DEFAULT_PREDICATE = () => true\n\n/**\n * Parses a stream of zipped bytes into FileEntry informations.\n *\n * @param stream A stream of zip file bytes.\n * @param predicate Optional. A function that returns true if the file should be downloaded.\n * @returns An iterable stream of FileEntry objects.\n */\nexport function streamZippedFileEntries(\n  stream: ReadableStream<Uint8Array>,\n  predicate: (\n    dirEntry: CentralDirectoryEntry | FileEntry,\n  ) => boolean = DEFAULT_PREDICATE,\n) {\n  const entriesStream = new ReadableStream<ZipEntry>({\n    async pull(controller) {\n      const entry = await nextZipEntry(stream)\n      if (!entry) {\n        controller.close()\n        return\n      }\n      controller.enqueue(entry)\n    },\n  }) as IterableReadableStream<ZipEntry>\n\n  return entriesStream\n    .pipeThrough(filterStream(({ signature }) => signature === SIGNATURE_FILE))\n    .pipeThrough(\n      filterStream(predicate as any),\n    ) as IterableReadableStream<FileEntry>\n}\n\n/**\n * Reads the next zip entry from a stream of zip file bytes.\n *\n * @param stream A stream of zip file bytes.\n * @returns A FileEntry object.\n */\nasync function nextZipEntry(stream: ReadableStream<Uint8Array>) {\n  const sigData = new DataView((await collectBytes(stream, 4))!.buffer)\n  const signature = sigData.getUint32(0, true)\n  if (signature === SIGNATURE_FILE) {\n    return await readFileEntry(stream, true)\n  } else if (signature === SIGNATURE_CENTRAL_DIRECTORY) {\n    return await readCentralDirectoryEntry(stream, true)\n  } else if (signature === SIGNATURE_CENTRAL_DIRECTORY_END) {\n    return await readEndCentralDirectoryEntry(stream, true)\n  }\n  return null\n}\n\n/**\n * Reads a file entry from a zip file.\n *\n * The file entry is structured as follows:\n *\n * ```\n * Offset\tBytes\tDescription\n *   0\t\t4\tLocal file header signature = 0x04034b50 (PK\u2665\u2666 or \"PK\\3\\4\")\n *   4\t\t2\tVersion needed to extract (minimum)\n *   6\t\t2\tGeneral purpose bit flag\n *   8\t\t2\tCompression method; e.g. none = 0, DEFLATE = 8 (or \"\\0x08\\0x00\")\n *   10\t\t2\tFile last modification time\n *   12\t\t2\tFile last modification date\n *   14\t\t4\tCRC-32 of uncompressed data\n *   18\t\t4\tCompressed size (or 0xffffffff for ZIP64)\n *   22\t\t4\tUncompressed size (or 0xffffffff for ZIP64)\n *   26\t\t2\tFile name length (n)\n *   28\t\t2\tExtra field length (m)\n *   30\t\tn\tFile name\n *   30+n\tm\tExtra field\n * ```\n *\n * @param stream\n * @param skipSignature Do not consume the signature from the stream.\n * @returns\n */\nexport async function readFileEntry(\n  stream: ReadableStream<Uint8Array>,\n  skipSignature = false,\n): Promise<FileEntry | null> {\n  if (!skipSignature) {\n    const sigData = new DataView((await collectBytes(stream, 4))!.buffer)\n    const signature = sigData.getUint32(0, true)\n    if (signature !== SIGNATURE_FILE) {\n      return null\n    }\n  }\n  const data = new DataView((await collectBytes(stream, 26))!.buffer)\n  const pathLength = data.getUint16(22, true)\n  const extraLength = data.getUint16(24, true)\n  const entry: Partial<FileEntry> = {\n    signature: SIGNATURE_FILE,\n    version: data.getUint32(0, true),\n    generalPurpose: data.getUint16(2, true),\n    compressionMethod: data.getUint16(4, true) as CompressionMethod,\n    lastModifiedTime: data.getUint16(6, true),\n    lastModifiedDate: data.getUint16(8, true),\n    crc: data.getUint32(10, true),\n    compressedSize: data.getUint32(14, true),\n    uncompressedSize: data.getUint32(18, true),\n  }\n\n  entry['path'] = await collectBytes(stream, pathLength)\n  entry['isDirectory'] = endsWithSlash(entry.path!)\n  entry['extra'] = await collectBytes(stream, extraLength)\n\n  // Make sure we consume the body stream or else\n  // we'll start reading the next file at the wrong\n  // offset.\n  // @TODO: Expose the body stream instead of reading it all\n  //        eagerly. Ensure the next iteration exhausts\n  //        the last body stream before moving on.\n\n  let bodyStream = limitBytes(stream, entry['compressedSize']!)\n\n  if (entry['compressionMethod'] === COMPRESSION_DEFLATE) {\n    /**\n     * We want to write raw deflate-compressed bytes into our\n     * final ZIP file. CompressionStream supports \"deflate-raw\"\n     * compression, but not on Node.js v18.\n     *\n     * As a workaround, we use the \"gzip\" compression and add\n     * the header and footer bytes. It works, because \"gzip\"\n     * compression is the same as \"deflate\" compression plus\n     * the header and the footer.\n     *\n     * The header is 10 bytes long:\n     * - 2 magic bytes: 0x1f, 0x8b\n     * - 1 compression method: 0x08 (deflate)\n     * - 1 header flags\n     * - 4 mtime: 0x00000000 (no timestamp)\n     * - 1 compression flags\n     * - 1 OS: 0x03 (Unix)\n     *\n     * The footer is 8 bytes long:\n     * - 4 bytes for CRC32 of the uncompressed data\n     * - 4 bytes for ISIZE (uncompressed size modulo 2^32)\n     */\n    const header = new Uint8Array(10)\n    header.set([0x1f, 0x8b, 0x08])\n\n    const footer = new Uint8Array(8)\n    const footerView = new DataView(footer.buffer)\n    footerView.setUint32(0, entry.crc!, true)\n    footerView.setUint32(4, entry.uncompressedSize! % 2 ** 32, true)\n    bodyStream = bodyStream\n      .pipeThrough(prependBytes(header))\n      .pipeThrough(appendBytes(footer))\n      .pipeThrough(new DecompressionStream('gzip'))\n  }\n  entry['bytes'] = await bodyStream\n    .pipeThrough(concatBytes(entry['uncompressedSize']))\n    .getReader()\n    .read()\n    .then(({ value }) => value!)\n  return entry as FileEntry\n}\n\n/**\n * Reads a central directory entry from a zip file.\n *\n * The central directory entry is structured as follows:\n *\n * ```\n * Offset Bytes Description\n *   0\t\t4\tCentral directory file header signature = 0x02014b50\n *   4\t\t2\tVersion made by\n *   6\t\t2\tVersion needed to extract (minimum)\n *   8\t\t2\tGeneral purpose bit flag\n *   10\t\t2\tCompression method\n *   12\t\t2\tFile last modification time\n *   14\t\t2\tFile last modification date\n *   16\t\t4\tCRC-32 of uncompressed data\n *   20\t\t4\tCompressed size (or 0xffffffff for ZIP64)\n *   24\t\t4\tUncompressed size (or 0xffffffff for ZIP64)\n *   28\t\t2\tFile name length (n)\n *   30\t\t2\tExtra field length (m)\n *   32\t\t2\tFile comment length (k)\n *   34\t\t2\tDisk number where file starts (or 0xffff for ZIP64)\n *   36\t\t2\tInternal file attributes\n *   38\t\t4\tExternal file attributes\n *   42\t\t4\tRelative offset of local file header (or 0xffffffff for ZIP64). This is the number of bytes between the start of the first disk on which the file occurs, and the start of the local file header. This allows software reading the central directory to locate the position of the file inside the ZIP file.\n *   46\t\tn\tFile name\n *   46+n\tm\tExtra field\n *   46+n+m\tk\tFile comment\n * ```\n *\n * @param stream\n * @param skipSignature\n * @returns\n */\nexport async function readCentralDirectoryEntry(\n  stream: ReadableStream<Uint8Array>,\n  skipSignature = false,\n): Promise<CentralDirectoryEntry | null> {\n  if (!skipSignature) {\n    const sigData = new DataView((await collectBytes(stream, 4))!.buffer)\n    const signature = sigData.getUint32(0, true)\n    if (signature !== SIGNATURE_CENTRAL_DIRECTORY) {\n      return null\n    }\n  }\n  const data = new DataView((await collectBytes(stream, 42))!.buffer)\n  const pathLength = data.getUint16(24, true)\n  const extraLength = data.getUint16(26, true)\n  const fileCommentLength = data.getUint16(28, true)\n  const centralDirectory: Partial<CentralDirectoryEntry> = {\n    signature: SIGNATURE_CENTRAL_DIRECTORY,\n    versionCreated: data.getUint16(0, true),\n    versionNeeded: data.getUint16(2, true),\n    generalPurpose: data.getUint16(4, true),\n    compressionMethod: data.getUint16(6, true) as CompressionMethod,\n    lastModifiedTime: data.getUint16(8, true),\n    lastModifiedDate: data.getUint16(10, true),\n    crc: data.getUint32(12, true),\n    compressedSize: data.getUint32(16, true),\n    uncompressedSize: data.getUint32(20, true),\n    diskNumber: data.getUint16(30, true),\n    internalAttributes: data.getUint16(32, true),\n    externalAttributes: data.getUint32(34, true),\n    firstByteAt: data.getUint32(38, true),\n  }\n  centralDirectory['lastByteAt'] =\n    centralDirectory.firstByteAt! +\n    FILE_HEADER_SIZE +\n    pathLength +\n    fileCommentLength +\n    extraLength! +\n    centralDirectory.compressedSize! -\n    1\n\n  centralDirectory['path'] = await collectBytes(stream, pathLength)\n  centralDirectory['isDirectory'] = endsWithSlash(centralDirectory.path!)\n  centralDirectory['extra'] = await collectBytes(stream, extraLength)\n  centralDirectory['fileComment'] = await collectBytes(\n    stream,\n    fileCommentLength,\n  )\n  return centralDirectory as CentralDirectoryEntry\n}\n\nfunction endsWithSlash(path: Uint8Array) {\n  return path[path.byteLength - 1] == '/'.charCodeAt(0)\n}\n\n/**\n * Reads the end of central directory entry from a zip file.\n *\n * The end of central directory entry is structured as follows:\n *\n * ```\n * Offset\tBytes\tDescription[33]\n *   0\t\t 4\t\tEnd of central directory signature = 0x06054b50\n *   4\t\t 2\t\tNumber of this disk (or 0xffff for ZIP64)\n *   6\t\t 2\t\tDisk where central directory starts (or 0xffff for ZIP64)\n *   8\t\t 2\t\tNumber of central directory records on this disk (or 0xffff for ZIP64)\n *   10\t\t 2\t\tTotal number of central directory records (or 0xffff for ZIP64)\n *   12\t\t 4\t\tSize of central directory (bytes) (or 0xffffffff for ZIP64)\n *   16\t\t 4\t\tOffset of start of central directory, relative to start of archive (or 0xffffffff for ZIP64)\n *   20\t\t 2\t\tComment length (n)\n *   22\t\t n\t\tComment\n * ```\n *\n * @param stream\n * @param skipSignature\n * @returns\n */\nasync function readEndCentralDirectoryEntry(\n  stream: ReadableStream<Uint8Array>,\n  skipSignature = false,\n) {\n  if (!skipSignature) {\n    const sigData = new DataView((await collectBytes(stream, 4))!.buffer)\n    const signature = sigData.getUint32(0, true)\n    if (signature !== SIGNATURE_CENTRAL_DIRECTORY_END) {\n      return null\n    }\n  }\n  const data = new DataView((await collectBytes(stream, 18))!.buffer)\n  const endOfDirectory: Partial<CentralDirectoryEndEntry> = {\n    signature: SIGNATURE_CENTRAL_DIRECTORY_END,\n    numberOfDisks: data.getUint16(0, true),\n    centralDirectoryStartDisk: data.getUint16(2, true),\n    numberCentralDirectoryRecordsOnThisDisk: data.getUint16(4, true),\n    numberCentralDirectoryRecords: data.getUint16(6, true),\n    centralDirectorySize: data.getUint32(8, true),\n    centralDirectoryOffset: data.getUint32(12, true),\n  }\n  const commentLength = data.getUint16(16, true)\n  endOfDirectory['comment'] = await collectBytes(stream, commentLength)\n  return endOfDirectory as CentralDirectoryEndEntry\n}\n", "export interface SemaphoreOptions {\n  concurrency: number\n}\n\nexport default class Semaphore {\n  private _running = 0\n  private concurrency: number\n  private queue: (() => void)[]\n\n  constructor({ concurrency }: SemaphoreOptions) {\n    this.concurrency = concurrency\n    this.queue = []\n  }\n\n  get running(): number {\n    return this._running\n  }\n\n  async acquire(): Promise<() => void> {\n    while (true) {\n      if (this._running >= this.concurrency) {\n        // Concurrency exhausted \u2013 wait until a lock is released:\n        await new Promise<void>((resolve) => this.queue.push(resolve))\n      } else {\n        // Acquire the lock:\n        this._running++\n        let released = false\n        return () => {\n          if (released) {\n            return\n          }\n          released = true\n          this._running--\n          // Release the lock:\n          if (this.queue.length > 0) {\n            this.queue.shift()!()\n          }\n        }\n      }\n    }\n  }\n\n  async run<T>(fn: () => T | Promise<T>): Promise<T> {\n    const release = await this.acquire()\n    try {\n      return await fn()\n    } finally {\n      release()\n    }\n  }\n}\n", "import Semaphore from './semaphore'\nimport { filterStream } from '../utils/filter-stream'\nimport { concatUint8Array } from '../utils/concat-uint8-array'\nimport { collectBytes } from '../utils/collect-bytes'\nimport {\n  readCentralDirectoryEntry,\n  readFileEntry,\n  decodeZip,\n} from './decode-zip'\nimport { CentralDirectoryEntry, FileEntry } from './types'\nimport { SIGNATURE_CENTRAL_DIRECTORY_END } from './types'\nimport { IterableReadableStream } from '../utils/iterable-stream-polyfill'\n\nconst CENTRAL_DIRECTORY_END_SCAN_CHUNK_SIZE = 110 * 1024\nconst BATCH_DOWNLOAD_OF_FILES_IF_CLOSER_THAN = 10 * 1024\nconst PREFER_RANGES_IF_FILE_LARGER_THAN = 1024 * 1024 * 1\nconst fetchSemaphore = new Semaphore({ concurrency: 10 })\n\nconst DEFAULT_PREDICATE = () => true\n\n/**\n * Streams the contents of a remote zip file.\n *\n * If the zip is large and the predicate is filtering the zip contents,\n * only the matching files will be downloaded using the Range header\n * (if supported by the server).\n *\n * @param url The URL of the zip file.\n * @param predicate Optional. A function that returns true if the file should be downloaded.\n * @returns A stream of zip entries.\n */\nexport async function decodeRemoteZip(\n  url: string,\n  predicate: (\n    dirEntry: CentralDirectoryEntry | FileEntry,\n  ) => boolean = DEFAULT_PREDICATE,\n) {\n  if (predicate === DEFAULT_PREDICATE) {\n    // If we're not filtering the zip contents, let's just\n    // grab the entire zip.\n    const response = await fetch(url)\n    return decodeZip(response.body!)\n  }\n\n  const contentLength = await fetchContentLength(url)\n  if (contentLength <= PREFER_RANGES_IF_FILE_LARGER_THAN) {\n    // If the zip is small enough, let's just grab it.\n    const response = await fetch(url)\n    return decodeZip(response.body!)\n  }\n\n  // Ensure ranges query support:\n  // Fetch one byte\n  const response = await fetch(url, {\n    headers: {\n      // 0-0 looks weird, doesn't it?\n      // The Range header is inclusive so it's actually\n      // a valid header asking for the first byte.\n      Range: 'bytes=0-0',\n      'Accept-Encoding': 'none',\n    },\n  })\n\n  // Fork the stream so that we can reuse it in case\n  // the Range header is unsupported and we're now streaming\n  // the entire file\n  const [peekStream, responseStream] = response.body!.tee()\n\n  // Read from the forked stream and close it.\n  const peekReader = peekStream.getReader()\n  const { value: peekBytes } = await peekReader.read()\n  const { done: peekDone } = await peekReader.read()\n  peekReader.releaseLock()\n  peekStream.cancel()\n\n  // Confirm our Range query worked as intended:\n  const rangesSupported = peekBytes?.length === 1 && peekDone\n  if (!rangesSupported) {\n    // Uh-oh, we're actually streaming the entire file.\n    // Let's reuse the forked stream as our response stream.\n    return decodeZip(responseStream)\n  }\n\n  // We're good, let's clean up the other branch of the response stream.\n  responseStream.cancel()\n  const source = await createFetchSource(url, contentLength)\n  return streamCentralDirectoryEntries(source)\n    .pipeThrough(filterStream(predicate))\n    .pipeThrough(partitionNearbyEntries())\n    .pipeThrough(\n      fetchPartitionedEntries(source),\n    ) as IterableReadableStream<FileEntry>\n}\n\n/**\n * Streams the central directory entries of a zip file.\n *\n * @param source\n * @returns\n */\nfunction streamCentralDirectoryEntries(source: BytesSource) {\n  let centralDirectoryStream: ReadableStream<Uint8Array>\n\n  return new ReadableStream<CentralDirectoryEntry>({\n    async start() {\n      centralDirectoryStream = await streamCentralDirectoryBytes(source)\n    },\n    async pull(controller) {\n      const entry = await readCentralDirectoryEntry(centralDirectoryStream)\n      if (!entry) {\n        controller.close()\n        return\n      }\n      controller.enqueue(entry)\n    },\n  })\n}\n\n/**\n * Streams the central directory bytes of a zip file.\n *\n * @param source\n * @returns\n */\nasync function streamCentralDirectoryBytes(source: BytesSource) {\n  const chunkSize = CENTRAL_DIRECTORY_END_SCAN_CHUNK_SIZE\n  let centralDirectory: Uint8Array = new Uint8Array()\n\n  let chunkStart = source.length\n  do {\n    chunkStart = Math.max(0, chunkStart - chunkSize)\n    const chunkEnd = Math.min(chunkStart + chunkSize - 1, source.length - 1)\n    const bytes = await collectBytes(\n      await source.streamBytes(chunkStart, chunkEnd),\n    )\n    centralDirectory = concatUint8Array(bytes!, centralDirectory)\n\n    // Scan the buffer for the signature\n    const view = new DataView(bytes!.buffer)\n    for (let i = view.byteLength - 4; i >= 0; i--) {\n      if (view.getUint32(i, true) !== SIGNATURE_CENTRAL_DIRECTORY_END) {\n        continue\n      }\n\n      // Confirm we have enough data to read the offset and the\n      // length of the central directory.\n      const centralDirectoryLengthAt = i + 12\n      const centralDirectoryOffsetAt = centralDirectoryLengthAt + 4\n      if (centralDirectory.byteLength < centralDirectoryOffsetAt + 4) {\n        throw new Error('Central directory not found')\n      }\n\n      // Read where the central directory starts\n      const dirStart = view.getUint32(centralDirectoryOffsetAt, true)\n      if (dirStart < chunkStart) {\n        // We're missing some bytes, let's grab them\n        const missingBytes = await collectBytes(\n          await source.streamBytes(dirStart, chunkStart - 1),\n        )\n        centralDirectory = concatUint8Array(missingBytes!, centralDirectory)\n      } else if (dirStart > chunkStart) {\n        // We've read too many bytes, let's trim them\n        centralDirectory = centralDirectory.slice(dirStart - chunkStart)\n      }\n      return new Blob([centralDirectory]).stream()\n    }\n  } while (chunkStart >= 0)\n\n  throw new Error('Central directory not found')\n}\n\n/**\n * Partitions files that are no further apart in the zip\n * archive than BATCH_DOWNLOAD_OF_FILES_IF_CLOSER_THAN.\n * It may download some extra files living within the gaps\n * between the partitions.\n */\nfunction partitionNearbyEntries() {\n  let lastFileEndsAt = 0\n  let currentChunk: CentralDirectoryEntry[] = []\n  return new TransformStream<CentralDirectoryEntry, CentralDirectoryEntry[]>({\n    transform(zipEntry, controller) {\n      // Byte distance too large, flush and start a new chunk\n      if (\n        zipEntry.firstByteAt >\n        lastFileEndsAt + BATCH_DOWNLOAD_OF_FILES_IF_CLOSER_THAN\n      ) {\n        controller.enqueue(currentChunk)\n        currentChunk = []\n      }\n      lastFileEndsAt = zipEntry.lastByteAt\n      currentChunk.push(zipEntry)\n    },\n    flush(controller) {\n      controller.enqueue(currentChunk)\n    },\n  })\n}\n\n/**\n * Fetches a chunk of files from the zip archive.\n *\n * If any extra files are present in the received\n * bytes stream, they are filtered out.\n */\nfunction fetchPartitionedEntries(\n  source: BytesSource,\n): ReadableWritablePair<FileEntry, CentralDirectoryEntry[]> {\n  /**\n   * This function implements a ReadableStream and a WritableStream\n   * instead of a TransformStream. This is intentional.\n   *\n   * In TransformStream, the `transform` function may return a\n   * promise. The next call to `transform` will be delayed until\n   * the promise resolves. This is a problem for us because we\n   * want to issue many fetch() requests in parallel.\n   *\n   * The only way to do that seems to be creating separate ReadableStream\n   * and WritableStream implementations.\n   */\n  let isWritableClosed = false\n  let requestsInProgress = 0\n  let readableController: ReadableStreamDefaultController<FileEntry>\n  const byteStreams: Array<\n    [CentralDirectoryEntry[], ReadableStream<Uint8Array>]\n  > = []\n  /**\n   * Receives chunks of CentralDirectoryEntries, and fetches\n   * the corresponding byte ranges from the remote zip file.\n   */\n  const writable = new WritableStream<CentralDirectoryEntry[]>({\n    write(zipEntries, controller) {\n      if (!zipEntries.length) {\n        return\n      }\n      ++requestsInProgress\n      // If the write() method returns a promise, the next\n      // call will be delayed until the promise resolves.\n      // Let's not return the promise, then.\n      // This will effectively issue many requests in parallel.\n      requestChunkRange(source, zipEntries)\n        .then((byteStream) => {\n          byteStreams.push([zipEntries, byteStream])\n        })\n        .catch((e) => {\n          controller.error(e)\n        })\n        .finally(() => {\n          --requestsInProgress\n        })\n    },\n    abort() {\n      isWritableClosed = true\n      readableController.close()\n    },\n    async close() {\n      isWritableClosed = true\n    },\n  })\n  /**\n   * Decodes zipped bytes into FileEntry objects.\n   */\n  const readable = new ReadableStream<FileEntry>({\n    start(controller) {\n      readableController = controller\n    },\n    async pull(controller) {\n      while (true) {\n        const allChunksProcessed =\n          isWritableClosed && !byteStreams.length && requestsInProgress === 0\n        if (allChunksProcessed) {\n          controller.close()\n          return\n        }\n\n        // There's no bytes available, but the writable\n        // stream is still open or there are still requests\n        // in progress. Let's wait for more bytes.\n        const waitingForMoreBytes = !byteStreams.length\n        if (waitingForMoreBytes) {\n          await new Promise((resolve) => setTimeout(resolve, 50))\n          continue\n        }\n\n        const [requestedPaths, stream] = byteStreams[0]\n        const file = await readFileEntry(stream)\n        // The stream is exhausted, let's remove it from the queue\n        // and try the next one.\n        const streamExhausted = !file\n        if (streamExhausted) {\n          byteStreams.shift()\n          continue\n        }\n\n        // There may be some extra files between the ones we're\n        // interested in. Let's filter out any files that got\n        // intertwined in the byte stream.\n        const isOneOfRequestedPaths = requestedPaths.find(\n          (entry) => entry.path === file.path,\n        )\n        if (!isOneOfRequestedPaths) {\n          continue\n        }\n\n        // Finally! We've got a file we're interested in.\n        controller.enqueue(file)\n        break\n      }\n    },\n  })\n\n  return {\n    readable,\n    writable,\n  }\n}\n\n/**\n * Requests a chunk of bytes from the bytes source.\n *\n * @param source\n * @param zipEntries\n */\nasync function requestChunkRange(\n  source: BytesSource,\n  zipEntries: CentralDirectoryEntry[],\n) {\n  const release = await fetchSemaphore.acquire()\n  try {\n    const lastZipEntry = zipEntries[zipEntries.length - 1]\n    const substream = await source.streamBytes(\n      zipEntries[0].firstByteAt,\n      lastZipEntry.lastByteAt,\n    )\n    return substream\n  } finally {\n    release()\n  }\n}\n\n/**\n * Fetches the Content-Length header from a remote URL.\n */\nasync function fetchContentLength(url: string) {\n  return await fetch(url, { method: 'HEAD' })\n    .then((response) => response.headers.get('Content-Length'))\n    .then((contentLength) => {\n      if (!contentLength) {\n        throw new Error('Content-Length header is missing')\n      }\n\n      const parsedLength = parseInt(contentLength, 10)\n      if (isNaN(parsedLength) || parsedLength < 0) {\n        throw new Error('Content-Length header is invalid')\n      }\n      return parsedLength\n    })\n}\n\n/**\n * Private and experimental API: Range-based data sources.\n *\n * The idea is that if we can read arbitrary byte ranges from\n * a file, we can retrieve a specific subset of a zip file.\n */\ntype BytesSource = {\n  length: number\n  streamBytes: (\n    start: number,\n    end: number,\n  ) => Promise<ReadableStream<Uint8Array>>\n}\n\n/**\n * Creates a BytesSource enabling fetching ranges of bytes\n * from a remote URL.\n */\nasync function createFetchSource(\n  url: string,\n  contentLength?: number,\n): Promise<BytesSource> {\n  if (contentLength === undefined) {\n    contentLength = await fetchContentLength(url)\n  }\n\n  return {\n    length: contentLength,\n    streamBytes: async (from: number, to: number) =>\n      await fetch(url, {\n        headers: {\n          // The Range header is inclusive, so we need to subtract 1\n          Range: `bytes=${from}-${to - 1}`,\n          'Accept-Encoding': 'none',\n        },\n      }).then((response) => response.body!),\n  }\n}\n", "import {\n  COMPRESSION_DEFLATE,\n  COMPRESSION_NONE,\n  CentralDirectoryEndEntry,\n  CentralDirectoryEntry,\n  FileHeader,\n} from './types'\nimport {\n  SIGNATURE_CENTRAL_DIRECTORY_END,\n  SIGNATURE_CENTRAL_DIRECTORY,\n  SIGNATURE_FILE,\n} from './types'\nimport { iteratorToStream } from '../utils/iterator-to-stream'\nimport { collectBytes } from '../utils/collect-bytes'\n\n/**\n * Compresses the given files into a ZIP archive.\n *\n * @param files - An async or sync iterable of files to be compressed.\n * @returns A readable stream of the compressed ZIP archive as Uint8Array chunks.\n */\nexport function encodeZip(\n  files: AsyncIterable<File> | Iterable<File>,\n): ReadableStream<Uint8Array> {\n  return iteratorToStream(files).pipeThrough(encodeZipTransform())\n}\n\n/**\n * Encodes the files into a ZIP format.\n *\n * @returns A stream transforming File objects into zipped bytes.\n */\nfunction encodeZipTransform() {\n  const offsetToFileHeaderMap: Map<number, FileHeader> = new Map()\n  let writtenBytes = 0\n  return new TransformStream<File, Uint8Array>({\n    async transform(file, controller) {\n      const entryBytes = new Uint8Array(await file.arrayBuffer())\n      /**\n       * We want to write raw deflate-compressed bytes into our\n       * final ZIP file. CompressionStream supports \"deflate-raw\"\n       * compression, but not on Node.js v18.\n       *\n       * As a workaround, we use the \"gzip\" compression and add\n       * the header and footer bytes. It works, because \"gzip\"\n       * compression is the same as \"deflate\" compression plus\n       * the header and the footer.\n       *\n       * The header is 10 bytes long:\n       * - 2 magic bytes: 0x1f, 0x8b\n       * - 1 compression method: 0x08 (deflate)\n       * - 1 header flags\n       * - 4 mtime: 0x00000000 (no timestamp)\n       * - 1 compression flags\n       * - 1 OS: 0x03 (Unix)\n       *\n       * The footer is 8 bytes long:\n       * - 4 bytes for CRC32 of the uncompressed data\n       * - 4 bytes for ISIZE (uncompressed size modulo 2^32)\n       */\n      let compressed = (await collectBytes(\n        new Blob([entryBytes])\n          .stream()\n          .pipeThrough(new CompressionStream('gzip')),\n      ))!\n      // Grab the CRC32 hash from the footer.\n      const crcHash = new DataView(compressed.buffer).getUint32(\n        compressed.byteLength - 8,\n        true,\n      )\n      // Strip the header and the footer.\n      compressed = compressed.slice(10, compressed.byteLength - 8)\n\n      const encodedPath = new TextEncoder().encode(file.name)\n      const zipFileEntry: FileHeader = {\n        signature: SIGNATURE_FILE,\n        version: 2,\n        generalPurpose: 0,\n        compressionMethod:\n          file.type === 'directory' || compressed.byteLength === 0\n            ? COMPRESSION_NONE\n            : COMPRESSION_DEFLATE,\n        lastModifiedTime: 0,\n        lastModifiedDate: 0,\n        crc: crcHash,\n        compressedSize: compressed.byteLength,\n        uncompressedSize: entryBytes.byteLength,\n        path: encodedPath,\n        extra: new Uint8Array(0),\n      }\n      offsetToFileHeaderMap.set(writtenBytes, zipFileEntry)\n\n      const headerBytes = encodeFileEntryHeader(zipFileEntry)\n      controller.enqueue(headerBytes)\n      writtenBytes += headerBytes.byteLength\n\n      controller.enqueue(compressed)\n      writtenBytes += compressed.byteLength\n    },\n    flush(controller) {\n      const centralDirectoryOffset = writtenBytes\n      let centralDirectorySize = 0\n      for (const [fileOffset, header] of offsetToFileHeaderMap.entries()) {\n        const centralDirectoryEntry: Partial<CentralDirectoryEntry> = {\n          ...header,\n          signature: SIGNATURE_CENTRAL_DIRECTORY,\n          fileComment: new Uint8Array(0),\n          diskNumber: 1,\n          internalAttributes: 0,\n          externalAttributes: 0,\n          firstByteAt: fileOffset,\n        }\n        const centralDirectoryEntryBytes = encodeCentralDirectoryEntry(\n          centralDirectoryEntry as CentralDirectoryEntry,\n          fileOffset,\n        )\n        controller.enqueue(centralDirectoryEntryBytes)\n        centralDirectorySize += centralDirectoryEntryBytes.byteLength\n      }\n      const centralDirectoryEnd: CentralDirectoryEndEntry = {\n        signature: SIGNATURE_CENTRAL_DIRECTORY_END,\n        numberOfDisks: 1,\n        centralDirectoryOffset,\n        centralDirectorySize,\n        centralDirectoryStartDisk: 1,\n        numberCentralDirectoryRecordsOnThisDisk: offsetToFileHeaderMap.size,\n        numberCentralDirectoryRecords: offsetToFileHeaderMap.size,\n        comment: new Uint8Array(0),\n      }\n      const centralDirectoryEndBytes =\n        encodeCentralDirectoryEnd(centralDirectoryEnd)\n      controller.enqueue(centralDirectoryEndBytes)\n      offsetToFileHeaderMap.clear()\n    },\n  })\n}\n\n/**\n * Encodes a file entry header as a Uint8Array.\n *\n * The array is structured as follows:\n *\n * ```\n * Offset\tBytes\tDescription\n *   0\t\t4\tLocal file header signature = 0x04034b50 (PK\u2665\u2666 or \"PK\\3\\4\")\n *   4\t\t2\tVersion needed to extract (minimum)\n *   6\t\t2\tGeneral purpose bit flag\n *   8\t\t2\tCompression method; e.g. none = 0, DEFLATE = 8 (or \"\\0x08\\0x00\")\n *   10\t\t2\tFile last modification time\n *   12\t\t2\tFile last modification date\n *   14\t\t4\tCRC-32 of uncompressed data\n *   18\t\t4\tCompressed size (or 0xffffffff for ZIP64)\n *   22\t\t4\tUncompressed size (or 0xffffffff for ZIP64)\n *   26\t\t2\tFile name length (n)\n *   28\t\t2\tExtra field length (m)\n *   30\t\tn\tFile name\n *   30+n\tm\tExtra field\n * ```\n */\nfunction encodeFileEntryHeader(entry: FileHeader) {\n  const buffer = new ArrayBuffer(\n    30 + entry.path.byteLength + entry.extra.byteLength,\n  )\n  const view = new DataView(buffer)\n  view.setUint32(0, entry.signature, true)\n  view.setUint16(4, entry.version, true)\n  view.setUint16(6, entry.generalPurpose, true)\n  view.setUint16(8, entry.compressionMethod, true)\n  view.setUint16(10, entry.lastModifiedDate, true)\n  view.setUint16(12, entry.lastModifiedTime, true)\n  view.setUint32(14, entry.crc, true)\n  view.setUint32(18, entry.compressedSize, true)\n  view.setUint32(22, entry.uncompressedSize, true)\n  view.setUint16(26, entry.path.byteLength, true)\n  view.setUint16(28, entry.extra.byteLength, true)\n  const uint8Header = new Uint8Array(buffer)\n  uint8Header.set(entry.path, 30)\n  uint8Header.set(entry.extra, 30 + entry.path.byteLength)\n  return uint8Header\n}\n\n/**\n * Encodes a central directory entry as a Uint8Array.\n *\n * The central directory entry is structured as follows:\n *\n * ```\n * Offset Bytes Description\n *   0\t\t4\tCentral directory file header signature = 0x02014b50\n *   4\t\t2\tVersion made by\n *   6\t\t2\tVersion needed to extract (minimum)\n *   8\t\t2\tGeneral purpose bit flag\n *   10\t\t2\tCompression method\n *   12\t\t2\tFile last modification time\n *   14\t\t2\tFile last modification date\n *   16\t\t4\tCRC-32 of uncompressed data\n *   20\t\t4\tCompressed size (or 0xffffffff for ZIP64)\n *   24\t\t4\tUncompressed size (or 0xffffffff for ZIP64)\n *   28\t\t2\tFile name length (n)\n *   30\t\t2\tExtra field length (m)\n *   32\t\t2\tFile comment length (k)\n *   34\t\t2\tDisk number where file starts (or 0xffff for ZIP64)\n *   36\t\t2\tInternal file attributes\n *   38\t\t4\tExternal file attributes\n *   42\t\t4\tRelative offset of local file header (or 0xffffffff for ZIP64). This is the number of bytes between the start of the first disk on which the file occurs, and the start of the local file header. This allows software reading the central directory to locate the position of the file inside the ZIP file.\n *   46\t\tn\tFile name\n *   46+n\tm\tExtra field\n *   46+n+m\tk\tFile comment\n * ```\n */\nfunction encodeCentralDirectoryEntry(\n  entry: CentralDirectoryEntry,\n  fileEntryOffset: number,\n) {\n  const buffer = new ArrayBuffer(\n    46 + entry.path.byteLength + entry.extra.byteLength,\n  )\n  const view = new DataView(buffer)\n  view.setUint32(0, entry.signature, true)\n  view.setUint16(4, entry.versionCreated, true)\n  view.setUint16(6, entry.versionNeeded, true)\n  view.setUint16(8, entry.generalPurpose, true)\n  view.setUint16(10, entry.compressionMethod, true)\n  view.setUint16(12, entry.lastModifiedDate, true)\n  view.setUint16(14, entry.lastModifiedTime, true)\n  view.setUint32(16, entry.crc, true)\n  view.setUint32(20, entry.compressedSize, true)\n  view.setUint32(24, entry.uncompressedSize, true)\n  view.setUint16(28, entry.path.byteLength, true)\n  view.setUint16(30, entry.extra.byteLength, true)\n  view.setUint16(32, entry.fileComment.byteLength, true)\n  view.setUint16(34, entry.diskNumber, true)\n  view.setUint16(36, entry.internalAttributes, true)\n  view.setUint32(38, entry.externalAttributes, true)\n  view.setUint32(42, fileEntryOffset, true)\n  const uint8Header = new Uint8Array(buffer)\n  uint8Header.set(entry.path, 46)\n  uint8Header.set(entry.extra, 46 + entry.path.byteLength)\n  return uint8Header\n}\n\n/**\n * Encodes the end of central directory entry as a Uint8Array.\n *\n * The end of central directory entry is structured as follows:\n *\n * ```\n * Offset\tBytes\tDescription[33]\n *   0\t\t 4\t\tEnd of central directory signature = 0x06054b50\n *   4\t\t 2\t\tNumber of this disk (or 0xffff for ZIP64)\n *   6\t\t 2\t\tDisk where central directory starts (or 0xffff for ZIP64)\n *   8\t\t 2\t\tNumber of central directory records on this disk (or 0xffff for ZIP64)\n *   10\t\t 2\t\tTotal number of central directory records (or 0xffff for ZIP64)\n *   12\t\t 4\t\tSize of central directory (bytes) (or 0xffffffff for ZIP64)\n *   16\t\t 4\t\tOffset of start of central directory, relative to start of archive (or 0xffffffff for ZIP64)\n *   20\t\t 2\t\tComment length (n)\n *   22\t\t n\t\tComment\n * ```\n */\nfunction encodeCentralDirectoryEnd(entry: CentralDirectoryEndEntry) {\n  const buffer = new ArrayBuffer(22 + entry.comment.byteLength)\n  const view = new DataView(buffer)\n  view.setUint32(0, entry.signature, true)\n  view.setUint16(4, entry.numberOfDisks, true)\n  view.setUint16(6, entry.centralDirectoryStartDisk, true)\n  view.setUint16(8, entry.numberCentralDirectoryRecordsOnThisDisk, true)\n  view.setUint16(10, entry.numberCentralDirectoryRecords, true)\n  view.setUint32(12, entry.centralDirectorySize, true)\n  view.setUint32(16, entry.centralDirectoryOffset, true)\n  view.setUint16(20, entry.comment.byteLength, true)\n  const uint8Header = new Uint8Array(buffer)\n  uint8Header.set(entry.comment, 22)\n  return uint8Header\n}\n", "import * as ZipStream from './index'\n\ndeclare var window: {\n  ZipStream: typeof ZipStream\n}\n\nwindow.ZipStream = ZipStream\n"],
  "mappings": "6GAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,kBAAAE,EAAA,iBAAAC,EAAA,gBAAAC,EAAA,oBAAAC,EAAA,cAAAC,EAAA,cAAAC,EAAA,qBAAAC,ICMO,SAASC,KAAoBC,EAAsB,CACxD,IAAMC,EAAS,IAAI,WACjBD,EAAO,OAAO,CAACE,EAAKC,IAAUD,EAAMC,EAAM,OAAQ,CAAC,CACrD,EACIC,EAAS,EACb,QAAWD,KAASH,EAClBC,EAAO,IAAIE,EAAOC,CAAM,EACxBA,GAAUD,EAAM,OAElB,OAAOF,CACT,CCNO,SAASI,EAAYC,EAAqB,CAC/C,GAAIA,IAAe,OAAW,CAC5B,IAAIC,EAAM,IAAI,WACd,OAAO,IAAI,gBAAwC,CACjD,UAAUC,EAAO,CACfD,EAAME,EAAiBF,EAAKC,CAAK,CACnC,EAEA,MAAME,EAAY,CAChBA,EAAW,QAAQH,CAAG,CACxB,CACF,CAAC,CACH,KAAO,CACL,IAAMI,EAAS,IAAI,YAAYL,GAAc,CAAC,EAC1CM,EAAS,EACb,OAAO,IAAI,gBAAwC,CACjD,UAAUJ,EAAO,CACF,IAAI,WAAWG,CAAM,EAC7B,IAAIH,EAAOI,CAAM,EACtBA,GAAUJ,EAAM,UAClB,EAEA,MAAME,EAAY,CAChBA,EAAW,QAAQ,IAAI,WAAWC,CAAM,CAAC,CAC3C,CACF,CAAC,CACH,CACF,CC9BO,SAASE,EAAWC,EAAoCC,EAAe,CAC5E,GAAIA,IAAU,EACZ,OAAO,IAAI,eAAe,CACxB,MAAMC,EAAY,CAChBA,EAAW,MAAM,CACnB,CACF,CAAC,EAEH,IAAMC,EAASH,EAAO,UAAU,CAAE,KAAM,MAAO,CAAC,EAC5CI,EAAS,EACb,OAAO,IAAI,eAAe,CACxB,MAAM,KAAKF,EAAY,CACrB,GAAM,CAAE,MAAAG,EAAO,KAAAC,CAAK,EAAI,MAAMH,EAAO,KAAK,IAAI,WAAWF,EAAQG,CAAM,CAAC,EACxE,GAAIE,EAAM,CACRH,EAAO,YAAY,EACnBD,EAAW,MAAM,EACjB,MACF,CACAE,GAAUC,EAAM,OAChBH,EAAW,QAAQG,CAAK,EAEpBD,GAAUH,IACZE,EAAO,YAAY,EACnBD,EAAW,MAAM,EAErB,EACA,QAAS,CACPC,EAAO,OAAO,CAChB,CACF,CAAC,CACH,CC3BA,eAAsBI,EACpBC,EACAC,EACA,CACA,OAAIA,IAAU,SACZD,EAASE,EAAWF,EAAQC,CAAK,GAG5B,MAAMD,EACV,YAAYG,EAAYF,CAAK,CAAC,EAC9B,UAAU,EACV,KAAK,EACL,KAAK,CAAC,CAAE,MAAAG,CAAM,IAAMA,CAAM,CAC/B,CCdA,eAAsBC,EACpBC,EACAC,EACA,CAEA,OAAO,IAAI,KAAK,CAAC,MAAMC,EAAaD,CAAM,CAAC,EAAGD,CAAQ,CACxD,CCPO,SAASG,EACdC,EAKA,CACA,GAAIA,aAA8B,eAChC,OAAOA,EAGT,IAAIC,EACJ,OAAI,OAAO,iBAAiBD,EAC1BC,EAAWD,EAAmB,OAAO,aAAa,EAAE,EAC3C,OAAO,YAAYA,EAC5BC,EAAWD,EAAmB,OAAO,QAAQ,EAAE,EAE/CC,EAAWD,EAGN,IAAI,eAAkB,CAC3B,MAAM,KAAKE,EAAY,CACrB,GAAM,CAAE,KAAAC,EAAM,MAAAC,CAAM,EAAI,MAAMH,EAAS,KAAK,EAC5C,GAAIE,EAAM,CACRD,EAAW,MAAM,EACjB,MACF,CACAA,EAAW,QAAQE,CAAK,CAC1B,CACF,CAAC,CACH,CChCO,IAAMC,EAAN,cAA2B,IAAK,CAQrC,YACUC,EACRC,EACAC,EACA,CACA,MAAM,CAAC,EAAGD,EAAM,CAAE,KAAAC,CAAK,CAAC,EAJhB,oBAAAF,CAKV,CAOS,OAAc,CACrB,MAAM,IAAI,MAAM,2CAA2C,CAC7D,CAOS,QAAS,CAChB,OAAO,KAAK,cACd,CAOA,MAAe,MAAO,CACpB,OAAO,IAAI,YAAY,EAAE,OAAO,MAAM,KAAK,YAAY,CAAC,CAC1D,CAOA,MAAe,aAAc,CAC3B,OAAQ,MAAMG,EAAa,KAAK,OAAO,CAAC,GAAG,MAC7C,CACF,EC7CK,eAAe,UAAU,OAAO,aAAa,IAEhD,eAAe,UAAU,OAAO,aAAa,EAAI,iBAAmB,CAClE,IAAMC,EAAS,KAAK,UAAU,EAC9B,GAAI,CACF,OAAa,CACX,GAAM,CAAE,KAAAC,EAAM,MAAAC,CAAM,EAAI,MAAMF,EAAO,KAAK,EAC1C,GAAIC,EACF,OAEF,MAAMC,CACR,CACF,QAAE,CACAF,EAAO,YAAY,CACrB,CACF,EAEA,eAAe,UAAU,QAEvB,eAAe,UAAU,OAAO,aAAa,GCzB1C,SAASG,EAAgBC,EAAkC,CAChE,OAAO,IAAI,gBAAsB,CAC/B,UAAUC,EAAOC,EAAY,CACvBF,EAAUC,CAAK,GACjBC,EAAW,QAAQD,CAAK,CAE5B,CACF,CAAC,CACH,CCRO,SAASE,EAAaC,EAAmB,CAC9C,IAAIC,EAAc,GAClB,OAAO,IAAI,gBAAwC,CACjD,MAAM,UAAUC,EAAOC,EAAY,CAC5BF,IACHA,EAAc,GACdE,EAAW,QAAQH,CAAK,GAE1BG,EAAW,QAAQD,CAAK,CAC1B,CACF,CAAC,CACH,CCXO,SAASE,EAAYC,EAAmB,CAC7C,OAAO,IAAI,gBAAwC,CACjD,MAAM,UAAUC,EAAOC,EAAY,CACjCA,EAAW,QAAQD,CAAK,CAC1B,EACA,MAAM,MAAMC,EAAY,CACtBA,EAAW,QAAQF,CAAK,CAC1B,CACF,CAAC,CACH,CCkBO,SAASG,EACdC,EACAC,EACA,CACA,OAAOC,EAAwBF,EAAQC,CAAS,EAAE,YAChD,IAAI,gBAAiC,CACnC,MAAM,UAAUE,EAAUC,EAAY,CACpC,IAAMC,EAAO,IAAI,KACf,CAACF,EAAS,KAAK,EACf,IAAI,YAAY,EAAE,OAAOA,EAAS,IAAI,EACtC,CACE,KAAMA,EAAS,YAAc,YAAc,MAC7C,CACF,EACAC,EAAW,QAAQC,CAAI,CACzB,CACF,CAAC,CACH,CACF,CAEA,IAAMC,EAAoB,IAAM,GASzB,SAASJ,EACdF,EACAC,EAEeK,EACf,CAYA,OAXsB,IAAI,eAAyB,CACjD,MAAM,KAAKF,EAAY,CACrB,IAAMG,EAAQ,MAAMC,EAAaR,CAAM,EACvC,GAAI,CAACO,EAAO,CACVH,EAAW,MAAM,EACjB,MACF,CACAA,EAAW,QAAQG,CAAK,CAC1B,CACF,CAAC,EAGE,YAAYE,EAAa,CAAC,CAAE,UAAAC,CAAU,IAAMA,IAAc,QAAc,CAAC,EACzE,YACCD,EAAaR,CAAgB,CAC/B,CACJ,CAQA,eAAeO,EAAaR,EAAoC,CAE9D,IAAMU,EADU,IAAI,UAAU,MAAMC,EAAaX,EAAQ,CAAC,GAAI,MAAM,EAC1C,UAAU,EAAG,EAAI,EAC3C,OAAIU,IAAc,SACT,MAAME,EAAcZ,EAAQ,EAAI,EAC9BU,IAAc,SAChB,MAAMG,EAA0Bb,EAAQ,EAAI,EAC1CU,IAAc,UAChB,MAAMI,EAA6Bd,EAAQ,EAAI,EAEjD,IACT,CA4BA,eAAsBY,EACpBZ,EACAe,EAAgB,GACW,CAC3B,GAAI,CAACA,GACa,IAAI,UAAU,MAAMJ,EAAaX,EAAQ,CAAC,GAAI,MAAM,EAC1C,UAAU,EAAG,EAAI,IACzB,SAChB,OAAO,KAGX,IAAMgB,EAAO,IAAI,UAAU,MAAML,EAAaX,EAAQ,EAAE,GAAI,MAAM,EAC5DiB,EAAaD,EAAK,UAAU,GAAI,EAAI,EACpCE,EAAcF,EAAK,UAAU,GAAI,EAAI,EACrCT,EAA4B,CAChC,UAAW,SACX,QAASS,EAAK,UAAU,EAAG,EAAI,EAC/B,eAAgBA,EAAK,UAAU,EAAG,EAAI,EACtC,kBAAmBA,EAAK,UAAU,EAAG,EAAI,EACzC,iBAAkBA,EAAK,UAAU,EAAG,EAAI,EACxC,iBAAkBA,EAAK,UAAU,EAAG,EAAI,EACxC,IAAKA,EAAK,UAAU,GAAI,EAAI,EAC5B,eAAgBA,EAAK,UAAU,GAAI,EAAI,EACvC,iBAAkBA,EAAK,UAAU,GAAI,EAAI,CAC3C,EAEAT,EAAM,KAAU,MAAMI,EAAaX,EAAQiB,CAAU,EACrDV,EAAM,YAAiBY,EAAcZ,EAAM,IAAK,EAChDA,EAAM,MAAW,MAAMI,EAAaX,EAAQkB,CAAW,EASvD,IAAIE,EAAaC,EAAWrB,EAAQO,EAAM,cAAkB,EAE5D,GAAIA,EAAM,oBAAyB,EAAqB,CAuBtD,IAAMe,EAAS,IAAI,WAAW,EAAE,EAChCA,EAAO,IAAI,CAAC,GAAM,IAAM,CAAI,CAAC,EAE7B,IAAMC,EAAS,IAAI,WAAW,CAAC,EACzBC,EAAa,IAAI,SAASD,EAAO,MAAM,EAC7CC,EAAW,UAAU,EAAGjB,EAAM,IAAM,EAAI,EACxCiB,EAAW,UAAU,EAAGjB,EAAM,iBAAoB,GAAK,GAAI,EAAI,EAC/Da,EAAaA,EACV,YAAYK,EAAaH,CAAM,CAAC,EAChC,YAAYI,EAAYH,CAAM,CAAC,EAC/B,YAAY,IAAI,oBAAoB,MAAM,CAAC,CAChD,CACA,OAAAhB,EAAM,MAAW,MAAMa,EACpB,YAAYO,EAAYpB,EAAM,gBAAmB,CAAC,EAClD,UAAU,EACV,KAAK,EACL,KAAK,CAAC,CAAE,MAAAqB,CAAM,IAAMA,CAAM,EACtBrB,CACT,CAmCA,eAAsBM,EACpBb,EACAe,EAAgB,GACuB,CACvC,GAAI,CAACA,GACa,IAAI,UAAU,MAAMJ,EAAaX,EAAQ,CAAC,GAAI,MAAM,EAC1C,UAAU,EAAG,EAAI,IACzB,SAChB,OAAO,KAGX,IAAMgB,EAAO,IAAI,UAAU,MAAML,EAAaX,EAAQ,EAAE,GAAI,MAAM,EAC5DiB,EAAaD,EAAK,UAAU,GAAI,EAAI,EACpCE,EAAcF,EAAK,UAAU,GAAI,EAAI,EACrCa,EAAoBb,EAAK,UAAU,GAAI,EAAI,EAC3Cc,EAAmD,CACvD,UAAW,SACX,eAAgBd,EAAK,UAAU,EAAG,EAAI,EACtC,cAAeA,EAAK,UAAU,EAAG,EAAI,EACrC,eAAgBA,EAAK,UAAU,EAAG,EAAI,EACtC,kBAAmBA,EAAK,UAAU,EAAG,EAAI,EACzC,iBAAkBA,EAAK,UAAU,EAAG,EAAI,EACxC,iBAAkBA,EAAK,UAAU,GAAI,EAAI,EACzC,IAAKA,EAAK,UAAU,GAAI,EAAI,EAC5B,eAAgBA,EAAK,UAAU,GAAI,EAAI,EACvC,iBAAkBA,EAAK,UAAU,GAAI,EAAI,EACzC,WAAYA,EAAK,UAAU,GAAI,EAAI,EACnC,mBAAoBA,EAAK,UAAU,GAAI,EAAI,EAC3C,mBAAoBA,EAAK,UAAU,GAAI,EAAI,EAC3C,YAAaA,EAAK,UAAU,GAAI,EAAI,CACtC,EACA,OAAAc,EAAiB,WACfA,EAAiB,YACjB,GACAb,EACAY,EACAX,EACAY,EAAiB,eACjB,EAEFA,EAAiB,KAAU,MAAMnB,EAAaX,EAAQiB,CAAU,EAChEa,EAAiB,YAAiBX,EAAcW,EAAiB,IAAK,EACtEA,EAAiB,MAAW,MAAMnB,EAAaX,EAAQkB,CAAW,EAClEY,EAAiB,YAAiB,MAAMnB,EACtCX,EACA6B,CACF,EACOC,CACT,CAEA,SAASX,EAAcY,EAAkB,CACvC,OAAOA,EAAKA,EAAK,WAAa,CAAC,GAAK,EACtC,CAwBA,eAAejB,EACbd,EACAe,EAAgB,GAChB,CACA,GAAI,CAACA,GACa,IAAI,UAAU,MAAMJ,EAAaX,EAAQ,CAAC,GAAI,MAAM,EAC1C,UAAU,EAAG,EAAI,IACzB,UAChB,OAAO,KAGX,IAAMgB,EAAO,IAAI,UAAU,MAAML,EAAaX,EAAQ,EAAE,GAAI,MAAM,EAC5DgC,EAAoD,CACxD,UAAW,UACX,cAAehB,EAAK,UAAU,EAAG,EAAI,EACrC,0BAA2BA,EAAK,UAAU,EAAG,EAAI,EACjD,wCAAyCA,EAAK,UAAU,EAAG,EAAI,EAC/D,8BAA+BA,EAAK,UAAU,EAAG,EAAI,EACrD,qBAAsBA,EAAK,UAAU,EAAG,EAAI,EAC5C,uBAAwBA,EAAK,UAAU,GAAI,EAAI,CACjD,EACMiB,EAAgBjB,EAAK,UAAU,GAAI,EAAI,EAC7C,OAAAgB,EAAe,QAAa,MAAMrB,EAAaX,EAAQiC,CAAa,EAC7DD,CACT,CCtVA,IAAqBE,EAArB,KAA+B,CACrB,SAAW,EACX,YACA,MAER,YAAY,CAAE,YAAAC,CAAY,EAAqB,CAC7C,KAAK,YAAcA,EACnB,KAAK,MAAQ,CAAC,CAChB,CAEA,IAAI,SAAkB,CACpB,OAAO,KAAK,QACd,CAEA,MAAM,SAA+B,CACnC,OACE,GAAI,KAAK,UAAY,KAAK,YAExB,MAAM,IAAI,QAAeC,GAAY,KAAK,MAAM,KAAKA,CAAO,CAAC,MACxD,CAEL,KAAK,WACL,IAAIC,EAAW,GACf,MAAO,IAAM,CACPA,IAGJA,EAAW,GACX,KAAK,WAED,KAAK,MAAM,OAAS,GACtB,KAAK,MAAM,MAAM,EAAG,EAExB,CACF,CAEJ,CAEA,MAAM,IAAOC,EAAsC,CACjD,IAAMC,EAAU,MAAM,KAAK,QAAQ,EACnC,GAAI,CACF,OAAO,MAAMD,EAAG,CAClB,QAAE,CACAC,EAAQ,CACV,CACF,CACF,ECrCA,IAAMC,EAAwC,IAAM,KAC9CC,EAAyC,GAAK,KAC9CC,EAAoC,KAAO,KAAO,EAClDC,EAAiB,IAAIC,EAAU,CAAE,YAAa,EAAG,CAAC,EAElDC,EAAoB,IAAM,GAahC,eAAsBC,EACpBC,EACAC,EAEeH,EACf,CACA,GAAIG,IAAcH,EAAmB,CAGnC,IAAMI,EAAW,MAAM,MAAMF,CAAG,EAChC,OAAOG,EAAUD,EAAS,IAAK,CACjC,CAEA,IAAME,EAAgB,MAAMC,EAAmBL,CAAG,EAClD,GAAII,GAAiBT,EAAmC,CAEtD,IAAMO,EAAW,MAAM,MAAMF,CAAG,EAChC,OAAOG,EAAUD,EAAS,IAAK,CACjC,CAIA,IAAMA,EAAW,MAAM,MAAMF,EAAK,CAChC,QAAS,CAIP,MAAO,YACP,kBAAmB,MACrB,CACF,CAAC,EAKK,CAACM,EAAYC,CAAc,EAAIL,EAAS,KAAM,IAAI,EAGlDM,EAAaF,EAAW,UAAU,EAClC,CAAE,MAAOG,CAAU,EAAI,MAAMD,EAAW,KAAK,EAC7C,CAAE,KAAME,CAAS,EAAI,MAAMF,EAAW,KAAK,EAMjD,GALAA,EAAW,YAAY,EACvBF,EAAW,OAAO,EAId,EADoBG,GAAW,SAAW,GAAKC,GAIjD,OAAOP,EAAUI,CAAc,EAIjCA,EAAe,OAAO,EACtB,IAAMI,EAAS,MAAMC,GAAkBZ,EAAKI,CAAa,EACzD,OAAOS,EAA8BF,CAAM,EACxC,YAAYG,EAAab,CAAS,CAAC,EACnC,YAAYc,EAAuB,CAAC,EACpC,YACCC,EAAwBL,CAAM,CAChC,CACJ,CAQA,SAASE,EAA8BF,EAAqB,CAC1D,IAAIM,EAEJ,OAAO,IAAI,eAAsC,CAC/C,MAAM,OAAQ,CACZA,EAAyB,MAAMC,EAA4BP,CAAM,CACnE,EACA,MAAM,KAAKQ,EAAY,CACrB,IAAMC,EAAQ,MAAMC,EAA0BJ,CAAsB,EACpE,GAAI,CAACG,EAAO,CACVD,EAAW,MAAM,EACjB,MACF,CACAA,EAAW,QAAQC,CAAK,CAC1B,CACF,CAAC,CACH,CAQA,eAAeF,EAA4BP,EAAqB,CAC9D,IAAMW,EAAY7B,EACd8B,EAA+B,IAAI,WAEnCC,EAAab,EAAO,OACxB,EAAG,CACDa,EAAa,KAAK,IAAI,EAAGA,EAAaF,CAAS,EAC/C,IAAMG,EAAW,KAAK,IAAID,EAAaF,EAAY,EAAGX,EAAO,OAAS,CAAC,EACjEe,EAAQ,MAAMC,EAClB,MAAMhB,EAAO,YAAYa,EAAYC,CAAQ,CAC/C,EACAF,EAAmBK,EAAiBF,EAAQH,CAAgB,EAG5D,IAAMM,EAAO,IAAI,SAASH,EAAO,MAAM,EACvC,QAASI,EAAID,EAAK,WAAa,EAAGC,GAAK,EAAGA,IAAK,CAC7C,GAAID,EAAK,UAAUC,EAAG,EAAI,IAAM,UAC9B,SAMF,IAAMC,EAD2BD,EAAI,GACuB,EAC5D,GAAIP,EAAiB,WAAaQ,EAA2B,EAC3D,MAAM,IAAI,MAAM,6BAA6B,EAI/C,IAAMC,EAAWH,EAAK,UAAUE,EAA0B,EAAI,EAC9D,GAAIC,EAAWR,EAAY,CAEzB,IAAMS,EAAe,MAAMN,EACzB,MAAMhB,EAAO,YAAYqB,EAAUR,EAAa,CAAC,CACnD,EACAD,EAAmBK,EAAiBK,EAAeV,CAAgB,CACrE,MAAWS,EAAWR,IAEpBD,EAAmBA,EAAiB,MAAMS,EAAWR,CAAU,GAEjE,OAAO,IAAI,KAAK,CAACD,CAAgB,CAAC,EAAE,OAAO,CAC7C,CACF,OAASC,GAAc,GAEvB,MAAM,IAAI,MAAM,6BAA6B,CAC/C,CAQA,SAAST,GAAyB,CAChC,IAAImB,EAAiB,EACjBC,EAAwC,CAAC,EAC7C,OAAO,IAAI,gBAAgE,CACzE,UAAUC,EAAUjB,EAAY,CAG5BiB,EAAS,YACTF,EAAiBxC,IAEjByB,EAAW,QAAQgB,CAAY,EAC/BA,EAAe,CAAC,GAElBD,EAAiBE,EAAS,WAC1BD,EAAa,KAAKC,CAAQ,CAC5B,EACA,MAAMjB,EAAY,CAChBA,EAAW,QAAQgB,CAAY,CACjC,CACF,CAAC,CACH,CAQA,SAASnB,EACPL,EAC0D,CAa1D,IAAI0B,EAAmB,GACnBC,EAAqB,EACrBC,EACEC,EAEF,CAAC,EAKCC,EAAW,IAAI,eAAwC,CAC3D,MAAMC,EAAYvB,EAAY,CACvBuB,EAAW,SAGhB,EAAEJ,EAKFK,EAAkBhC,EAAQ+B,CAAU,EACjC,KAAME,GAAe,CACpBJ,EAAY,KAAK,CAACE,EAAYE,CAAU,CAAC,CAC3C,CAAC,EACA,MAAOC,GAAM,CACZ1B,EAAW,MAAM0B,CAAC,CACpB,CAAC,EACA,QAAQ,IAAM,CACb,EAAEP,CACJ,CAAC,EACL,EACA,OAAQ,CACND,EAAmB,GACnBE,EAAmB,MAAM,CAC3B,EACA,MAAM,OAAQ,CACZF,EAAmB,EACrB,CACF,CAAC,EAqDD,MAAO,CACL,SAlDe,IAAI,eAA0B,CAC7C,MAAMlB,EAAY,CAChBoB,EAAqBpB,CACvB,EACA,MAAM,KAAKA,EAAY,CACrB,OAAa,CAGX,GADEkB,GAAoB,CAACG,EAAY,QAAUF,IAAuB,EAC5C,CACtBnB,EAAW,MAAM,EACjB,MACF,CAMA,GAD4B,CAACqB,EAAY,OAChB,CACvB,MAAM,IAAI,QAASM,GAAY,WAAWA,EAAS,EAAE,CAAC,EACtD,QACF,CAEA,GAAM,CAACC,EAAgBC,CAAM,EAAIR,EAAY,CAAC,EACxCS,EAAO,MAAMC,EAAcF,CAAM,EAIvC,GADwB,CAACC,EACJ,CACnBT,EAAY,MAAM,EAClB,QACF,CAQA,GAH8BO,EAAe,KAC1C3B,GAAUA,EAAM,OAAS6B,EAAK,IACjC,EAMA,CAAA9B,EAAW,QAAQ8B,CAAI,EACvB,MACF,CACF,CACF,CAAC,EAIC,SAAAR,CACF,CACF,CAQA,eAAeE,EACbhC,EACA+B,EACA,CACA,IAAMS,EAAU,MAAMvD,EAAe,QAAQ,EAC7C,GAAI,CACF,IAAMwD,EAAeV,EAAWA,EAAW,OAAS,CAAC,EAKrD,OAJkB,MAAM/B,EAAO,YAC7B+B,EAAW,CAAC,EAAE,YACdU,EAAa,UACf,CAEF,QAAE,CACAD,EAAQ,CACV,CACF,CAKA,eAAe9C,EAAmBL,EAAa,CAC7C,OAAO,MAAM,MAAMA,EAAK,CAAE,OAAQ,MAAO,CAAC,EACvC,KAAME,GAAaA,EAAS,QAAQ,IAAI,gBAAgB,CAAC,EACzD,KAAME,GAAkB,CACvB,GAAI,CAACA,EACH,MAAM,IAAI,MAAM,kCAAkC,EAGpD,IAAMiD,EAAe,SAASjD,EAAe,EAAE,EAC/C,GAAI,MAAMiD,CAAY,GAAKA,EAAe,EACxC,MAAM,IAAI,MAAM,kCAAkC,EAEpD,OAAOA,CACT,CAAC,CACL,CAoBA,eAAezC,GACbZ,EACAI,EACsB,CACtB,OAAIA,IAAkB,SACpBA,EAAgB,MAAMC,EAAmBL,CAAG,GAGvC,CACL,OAAQI,EACR,YAAa,MAAOkD,EAAcC,IAChC,MAAM,MAAMvD,EAAK,CACf,QAAS,CAEP,MAAO,SAASsD,CAAI,IAAIC,EAAK,CAAC,GAC9B,kBAAmB,MACrB,CACF,CAAC,EAAE,KAAMrD,GAAaA,EAAS,IAAK,CACxC,CACF,CCvXO,SAASsD,EACdC,EAC4B,CAC5B,OAAOC,EAAiBD,CAAK,EAAE,YAAYE,GAAmB,CAAC,CACjE,CAOA,SAASA,IAAqB,CAC5B,IAAMC,EAAiD,IAAI,IACvDC,EAAe,EACnB,OAAO,IAAI,gBAAkC,CAC3C,MAAM,UAAUC,EAAMC,EAAY,CAChC,IAAMC,EAAa,IAAI,WAAW,MAAMF,EAAK,YAAY,CAAC,EAuBtDG,EAAc,MAAMC,EACtB,IAAI,KAAK,CAACF,CAAU,CAAC,EAClB,OAAO,EACP,YAAY,IAAI,kBAAkB,MAAM,CAAC,CAC9C,EAEMG,EAAU,IAAI,SAASF,EAAW,MAAM,EAAE,UAC9CA,EAAW,WAAa,EACxB,EACF,EAEAA,EAAaA,EAAW,MAAM,GAAIA,EAAW,WAAa,CAAC,EAE3D,IAAMG,EAAc,IAAI,YAAY,EAAE,OAAON,EAAK,IAAI,EAChDO,EAA2B,CAC/B,UAAW,SACX,QAAS,EACT,eAAgB,EAChB,kBACEP,EAAK,OAAS,aAAeG,EAAW,aAAe,EACnD,EACA,EACN,iBAAkB,EAClB,iBAAkB,EAClB,IAAKE,EACL,eAAgBF,EAAW,WAC3B,iBAAkBD,EAAW,WAC7B,KAAMI,EACN,MAAO,IAAI,WAAW,CAAC,CACzB,EACAR,EAAsB,IAAIC,EAAcQ,CAAY,EAEpD,IAAMC,EAAcC,GAAsBF,CAAY,EACtDN,EAAW,QAAQO,CAAW,EAC9BT,GAAgBS,EAAY,WAE5BP,EAAW,QAAQE,CAAU,EAC7BJ,GAAgBI,EAAW,UAC7B,EACA,MAAMF,EAAY,CAChB,IAAMS,EAAyBX,EAC3BY,EAAuB,EAC3B,OAAW,CAACC,EAAYC,CAAM,IAAKf,EAAsB,QAAQ,EAAG,CAClE,IAAMgB,EAAwD,CAC5D,GAAGD,EACH,UAAW,SACX,YAAa,IAAI,WAAW,CAAC,EAC7B,WAAY,EACZ,mBAAoB,EACpB,mBAAoB,EACpB,YAAaD,CACf,EACMG,EAA6BC,GACjCF,EACAF,CACF,EACAX,EAAW,QAAQc,CAA0B,EAC7CJ,GAAwBI,EAA2B,UACrD,CACA,IAAME,EAAgD,CACpD,UAAW,UACX,cAAe,EACf,uBAAAP,EACA,qBAAAC,EACA,0BAA2B,EAC3B,wCAAyCb,EAAsB,KAC/D,8BAA+BA,EAAsB,KACrD,QAAS,IAAI,WAAW,CAAC,CAC3B,EACMoB,EACJC,GAA0BF,CAAmB,EAC/ChB,EAAW,QAAQiB,CAAwB,EAC3CpB,EAAsB,MAAM,CAC9B,CACF,CAAC,CACH,CAwBA,SAASW,GAAsBW,EAAmB,CAChD,IAAMC,EAAS,IAAI,YACjB,GAAKD,EAAM,KAAK,WAAaA,EAAM,MAAM,UAC3C,EACME,EAAO,IAAI,SAASD,CAAM,EAChCC,EAAK,UAAU,EAAGF,EAAM,UAAW,EAAI,EACvCE,EAAK,UAAU,EAAGF,EAAM,QAAS,EAAI,EACrCE,EAAK,UAAU,EAAGF,EAAM,eAAgB,EAAI,EAC5CE,EAAK,UAAU,EAAGF,EAAM,kBAAmB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,IAAK,EAAI,EAClCE,EAAK,UAAU,GAAIF,EAAM,eAAgB,EAAI,EAC7CE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,KAAK,WAAY,EAAI,EAC9CE,EAAK,UAAU,GAAIF,EAAM,MAAM,WAAY,EAAI,EAC/C,IAAMG,EAAc,IAAI,WAAWF,CAAM,EACzC,OAAAE,EAAY,IAAIH,EAAM,KAAM,EAAE,EAC9BG,EAAY,IAAIH,EAAM,MAAO,GAAKA,EAAM,KAAK,UAAU,EAChDG,CACT,CA+BA,SAASP,GACPI,EACAI,EACA,CACA,IAAMH,EAAS,IAAI,YACjB,GAAKD,EAAM,KAAK,WAAaA,EAAM,MAAM,UAC3C,EACME,EAAO,IAAI,SAASD,CAAM,EAChCC,EAAK,UAAU,EAAGF,EAAM,UAAW,EAAI,EACvCE,EAAK,UAAU,EAAGF,EAAM,eAAgB,EAAI,EAC5CE,EAAK,UAAU,EAAGF,EAAM,cAAe,EAAI,EAC3CE,EAAK,UAAU,EAAGF,EAAM,eAAgB,EAAI,EAC5CE,EAAK,UAAU,GAAIF,EAAM,kBAAmB,EAAI,EAChDE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,IAAK,EAAI,EAClCE,EAAK,UAAU,GAAIF,EAAM,eAAgB,EAAI,EAC7CE,EAAK,UAAU,GAAIF,EAAM,iBAAkB,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,KAAK,WAAY,EAAI,EAC9CE,EAAK,UAAU,GAAIF,EAAM,MAAM,WAAY,EAAI,EAC/CE,EAAK,UAAU,GAAIF,EAAM,YAAY,WAAY,EAAI,EACrDE,EAAK,UAAU,GAAIF,EAAM,WAAY,EAAI,EACzCE,EAAK,UAAU,GAAIF,EAAM,mBAAoB,EAAI,EACjDE,EAAK,UAAU,GAAIF,EAAM,mBAAoB,EAAI,EACjDE,EAAK,UAAU,GAAIE,EAAiB,EAAI,EACxC,IAAMD,EAAc,IAAI,WAAWF,CAAM,EACzC,OAAAE,EAAY,IAAIH,EAAM,KAAM,EAAE,EAC9BG,EAAY,IAAIH,EAAM,MAAO,GAAKA,EAAM,KAAK,UAAU,EAChDG,CACT,CAoBA,SAASJ,GAA0BC,EAAiC,CAClE,IAAMC,EAAS,IAAI,YAAY,GAAKD,EAAM,QAAQ,UAAU,EACtDE,EAAO,IAAI,SAASD,CAAM,EAChCC,EAAK,UAAU,EAAGF,EAAM,UAAW,EAAI,EACvCE,EAAK,UAAU,EAAGF,EAAM,cAAe,EAAI,EAC3CE,EAAK,UAAU,EAAGF,EAAM,0BAA2B,EAAI,EACvDE,EAAK,UAAU,EAAGF,EAAM,wCAAyC,EAAI,EACrEE,EAAK,UAAU,GAAIF,EAAM,8BAA+B,EAAI,EAC5DE,EAAK,UAAU,GAAIF,EAAM,qBAAsB,EAAI,EACnDE,EAAK,UAAU,GAAIF,EAAM,uBAAwB,EAAI,EACrDE,EAAK,UAAU,GAAIF,EAAM,QAAQ,WAAY,EAAI,EACjD,IAAMG,EAAc,IAAI,WAAWF,CAAM,EACzC,OAAAE,EAAY,IAAIH,EAAM,QAAS,EAAE,EAC1BG,CACT,CC3QA,OAAO,UAAYE",
  "names": ["src_exports", "__export", "StreamedFile", "collectBytes", "collectFile", "decodeRemoteZip", "decodeZip", "encodeZip", "iteratorToStream", "concatUint8Array", "arrays", "result", "sum", "array", "offset", "concatBytes", "totalBytes", "acc", "chunk", "concatUint8Array", "controller", "buffer", "offset", "limitBytes", "stream", "bytes", "controller", "reader", "offset", "value", "done", "collectBytes", "stream", "bytes", "limitBytes", "concatBytes", "value", "collectFile", "fileName", "stream", "collectBytes", "iteratorToStream", "iteratorOrIterable", "iterator", "controller", "done", "value", "StreamedFile", "readableStream", "name", "type", "collectBytes", "reader", "done", "value", "filterStream", "predicate", "chunk", "controller", "prependBytes", "bytes", "isPrepended", "chunk", "controller", "appendBytes", "bytes", "chunk", "controller", "decodeZip", "stream", "predicate", "streamZippedFileEntries", "zipEntry", "controller", "file", "DEFAULT_PREDICATE", "entry", "nextZipEntry", "filterStream", "signature", "collectBytes", "readFileEntry", "readCentralDirectoryEntry", "readEndCentralDirectoryEntry", "skipSignature", "data", "pathLength", "extraLength", "endsWithSlash", "bodyStream", "limitBytes", "header", "footer", "footerView", "prependBytes", "appendBytes", "concatBytes", "value", "fileCommentLength", "centralDirectory", "path", "endOfDirectory", "commentLength", "Semaphore", "concurrency", "resolve", "released", "fn", "release", "CENTRAL_DIRECTORY_END_SCAN_CHUNK_SIZE", "BATCH_DOWNLOAD_OF_FILES_IF_CLOSER_THAN", "PREFER_RANGES_IF_FILE_LARGER_THAN", "fetchSemaphore", "Semaphore", "DEFAULT_PREDICATE", "decodeRemoteZip", "url", "predicate", "response", "decodeZip", "contentLength", "fetchContentLength", "peekStream", "responseStream", "peekReader", "peekBytes", "peekDone", "source", "createFetchSource", "streamCentralDirectoryEntries", "filterStream", "partitionNearbyEntries", "fetchPartitionedEntries", "centralDirectoryStream", "streamCentralDirectoryBytes", "controller", "entry", "readCentralDirectoryEntry", "chunkSize", "centralDirectory", "chunkStart", "chunkEnd", "bytes", "collectBytes", "concatUint8Array", "view", "i", "centralDirectoryOffsetAt", "dirStart", "missingBytes", "lastFileEndsAt", "currentChunk", "zipEntry", "isWritableClosed", "requestsInProgress", "readableController", "byteStreams", "writable", "zipEntries", "requestChunkRange", "byteStream", "e", "resolve", "requestedPaths", "stream", "file", "readFileEntry", "release", "lastZipEntry", "parsedLength", "from", "to", "encodeZip", "files", "iteratorToStream", "encodeZipTransform", "offsetToFileHeaderMap", "writtenBytes", "file", "controller", "entryBytes", "compressed", "collectBytes", "crcHash", "encodedPath", "zipFileEntry", "headerBytes", "encodeFileEntryHeader", "centralDirectoryOffset", "centralDirectorySize", "fileOffset", "header", "centralDirectoryEntry", "centralDirectoryEntryBytes", "encodeCentralDirectoryEntry", "centralDirectoryEnd", "centralDirectoryEndBytes", "encodeCentralDirectoryEnd", "entry", "buffer", "view", "uint8Header", "fileEntryOffset", "src_exports"]
}
